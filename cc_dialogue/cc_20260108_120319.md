# Chat Dialogue Log

## 对话时间
2026-01-08 12:03:19

---

## 对话内容

### 问题 1
用户：verl/trainer/ppo/ray_trainer.py critic模型、ref模型分别是从什么模型初始化的？
助手：根据对 `ray_trainer.py` 和 `fsdp_workers.py` 以及配置文件 `ppo_trainer.yaml` 的分析：

### **Critic 模型**

Critic 模型从 **`critic.model.path`** 配置的路径初始化：

**位置**：`fsdp_workers.py` 第 578 行 和 620 行
```python
local_path = copy_local_path_from_hdfs(config.model.path)  # config.model.path = critic.model.path
...
critic_module = AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path=local_path, ...)
```

**默认配置** (`ppo_trainer.yaml` 第 101-103 行)：
```yaml
critic:
  model:
    path: ~/models/deepseek-llm-7b-chat  # 可以与 actor 不同
    tokenizer_path: ${actor_rollout_ref.model.path}  # tokenizer 默认用 actor 的
```

- Critic 使用 `AutoModelForTokenClassification` 加载，并添加一个分类头（`num_labels=1`）用于预测 value
- **Critic 可以从与 Actor 相同或不同的预训练模型初始化**

---

### **Ref (Reference Policy) 模型**

Ref 模型从 **与 Actor 完全相同的路径** `actor_rollout_ref.model.path` 初始化：

**位置**：`fsdp_workers.py` 第 334-341 行
```python
if self._is_ref:
    self.ref_module_fsdp = self._build_model_optimizer(
        model_path=self.config.model.path,  # 同 actor_rollout_ref.model.path
        ...
    )[0]
```

- Ref 模型使用与 Actor **完全相同的模型权重**初始化
- Ref 模型在训练期间**参数冻结**，不进行更新，用于计算 KL 惩罚的参考分布

---

### **总结**

| 模型 | 初始化来源 | 配置项 | 是否可训练 |
|------|-----------|--------|----------|
| **Actor** | `actor_rollout_ref.model.path` | 如 `~/models/deepseek-llm-7b-chat` | ✅ 是 |
| **Ref** | `actor_rollout_ref.model.path` (同 Actor) | 同上 | ❌ 否（冻结） |
| **Critic** | `critic.model.path` | 可与 Actor 相同或不同 | ✅ 是 |

在实际训练中，Critic 和 Ref 通常都从与 Actor 相同的预训练模型初始化，但 Critic 会额外添加一个 value head 并且在训练过程中更新参数。

---

### 问题 2
用户：如何给critic添加一个value head？给出代码示例
助手：在 VERL 框架中，给 Critic 添加 Value Head 的方法非常简洁，它利用了 Hugging Face Transformers 原生的 `AutoModelForTokenClassification` 类。

以下是两种实现方式的讲解：一种是 **VERL 使用的官方方式**，另一种是 **自定义 Value Head 的底层实现方式**（如果你需要更灵活的控制）。

### 1. VERL 官方实现方式（推荐）

VERL 在 `fsdp_workers.py` 中直接使用 `AutoModelForTokenClassification` 并将 `num_labels` 设置为 1。这会自动在模型的最后一层隐藏状态上添加一个线性层 `(hidden_size, 1)`，从而为每个 token 输出一个标量值（Value）。

**代码示例：**

```python
from transformers import AutoConfig, AutoModelForTokenClassification
import torch

def create_critic_model(model_path):
    # 1. 加载配置
    config = AutoConfig.from_pretrained(model_path)
    
    # 2. 关键步骤：设置 num_labels = 1
    # 这告诉 HF 我们需要一个输出维度为 1 的分类头，即回归头（Regression Head）
    config.num_labels = 1
    
    # 3. 可选：移除分类头的 Dropout，通常 Value 预测不需要 Dropout
    setattr(config, 'classifier_dropout', 0.)
    setattr(config, 'hidden_dropout', '0')
    
    # 4. 加载模型
    # HF 会自动初始化一个随机权重的 Linear 层作为 classifier
    critic_model = AutoModelForTokenClassification.from_pretrained(
        model_path,
        config=config,
        torch_dtype=torch.bfloat16
    )
    
    return critic_model

# 使用示例
# model = create_critic_model("deepseek-ai/deepseek-llm-7b-chat")
# inputs = tokenizer("Hello", return_tensors="pt")
# output = model(**inputs)
# value = output.logits # Shape: [batch_size, seq_len, 1]
```

### 2. 自定义 Value Head 实现（底层原理）

如果你想完全掌控 Value Head 的结构（例如添加 MLP 层而不仅仅是线性层），或是想了解其底层是如何工作的，可以创建一个自定义的 `nn.Module`。

这通常被称作 `ModelWithValueHead` 模式。

**代码示例：**

```python
import torch
import torch.nn as nn
from transformers import AutoModel

class AutoModelWithValueHead(nn.Module):
    def __init__(self, model_name_or_path):
        super().__init__()
        # 1. 加载基础模型 (Base Transformer)
        # 注意：这里用 AutoModel 而不是 AutoModelForCausalLM，因为我们需要原始的 hidden_states
        self.base_model = AutoModel.from_pretrained(model_name_or_path)
        
        # 获取隐藏层维度
        hidden_size = self.base_model.config.hidden_size
        
        # 2. 定义 Value Head
        # 一个简单的线性层，将 hidden_state 映射到 1 个标量
        self.value_head = nn.Linear(hidden_size, 1, bias=False)
        
        # 初始化权重 (通常是一个很小的值，以避免初始 Value 过大)
        nn.init.normal_(self.value_head.weight, std=0.01)

    def forward(self, input_ids, attention_mask=None):
        # 1. 基础模型前向传播
        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)
        
        # 获取最后一层的隐藏状态
        # Shape: [batch_size, seq_len, hidden_size]
        last_hidden_state = outputs.last_hidden_state
        
        # 2. Value Head 前向传播
        # Shape: [batch_size, seq_len, 1]
        values = self.value_head(last_hidden_state)
        
        # 移除最后一个维度，变为 [batch_size, seq_len]
        return values.squeeze(-1)

# 使用示例
# model = AutoModelWithValueHead("deepseek-ai/deepseek-llm-7b-chat")
# values = model(input_ids, attention_mask)
```

### 总结

*   **VERL 方式**: 使用 `AutoModelForTokenClassification(num_labels=1)`。优点是简单、兼容 FSDP、自动处理权重加载。
*   **你需要修改**: 如果你需要更复杂的 Value Head（例如 `Linear -> ReLU -> Linear`），则需要使用第二种方法封装模型类。但对于标准的 PPO，第一种方法是业界标准做法。

---
