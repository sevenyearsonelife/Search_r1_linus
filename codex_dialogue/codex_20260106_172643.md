# Chat Dialogue Log

## å¯¹è¯æ—¶é—´
2026-01-06 17:26:43

---

## å¯¹è¯å†…å®¹

### é—®é¢˜ 1
ç”¨æˆ·ï¼š@[verl/trainer/ppo/ray_trainer.py] 

## PPOè®­ç»ƒæµç¨‹å„æ¨¡å—è¾“å‡ºè¯¦è§£

å‡è®¾ prompt = `"ä½ çŸ¥é“1+1=2å—ï¼Ÿ"`ï¼ŒTokenizer å°†å…¶ç¼–ç ä¸º `input_ids = [1, 234, 567, 89, 10, 11, 12, 2]`ï¼ˆé•¿åº¦8ï¼‰

---

### Stage 1: Actor ç”Ÿæˆå“åº”

```python
batch = actor.generate_sequences(prompts)
```

**Actor è¾“å‡º `gen_batch_output`:**

| å­—æ®µ | å½¢çŠ¶ | ç¤ºä¾‹å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `input_ids` | `(batch, seq_len)` | `[[1,234,567,89,10,11,12,2,100,101,102,103,104,2]]` | å®Œæ•´åºåˆ—ï¼ˆprompt + responseï¼‰ |
| `attention_mask` | `(batch, seq_len)` | `[[1,1,1,1,1,1,1,1,1,1,1,1,1,1]]` | æœ‰æ•ˆtokenæ ‡è®° |
| `responses` | `(batch, response_len)` | `[[100,101,102,103,104,2]]` | ç”Ÿæˆçš„å“åº”ï¼ˆ"æ˜¯çš„ï¼Œ1+1=2"ï¼‰ |
| `old_log_probs` | `(batch, response_len)` | `[[-0.12,-0.08,-0.15,-0.10,-0.05,-0.02]]` | ç”Ÿæˆæ—¶æ¯ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡ |

---

### Stage 2: è®­ç»ƒæ•°æ®å‡†å¤‡

#### 2.1 Critic è®¡ç®—ä»·å€¼

```python
batch = critic.compute_values(batch)
```

**Critic è¾“å‡º `values`:**

| å­—æ®µ | å½¢çŠ¶ | ç¤ºä¾‹å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `values` | `(batch, response_len)` | `[[0.82, 0.85, 0.88, 0.90, 0.92, 0.95]]` | æ¯ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€ä»·å€¼ä¼°è®¡ |

---

#### 2.2 Reference Policy è®¡ç®—å¯¹æ•°æ¦‚ç‡

```python
batch = reference.compute_log_prob(batch)
```

**Reference è¾“å‡º `ref_log_prob`:**

| å­—æ®µ | å½¢çŠ¶ | ç¤ºä¾‹å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `ref_log_prob` | `(batch, response_len)` | `[[-0.15,-0.10,-0.18,-0.12,-0.08,-0.03]]` | å‚è€ƒç­–ç•¥ä¸‹çš„å¯¹æ•°æ¦‚ç‡ï¼Œç”¨äºè®¡ç®—KLæ•£åº¦ |

**KLæ•£åº¦è®¡ç®—**ï¼ˆåœ¨ `apply_kl_penalty` ä¸­ï¼‰:
```python
kld = old_log_probs - ref_log_prob  # é€token KL
# = [-0.12-(-0.15), -0.08-(-0.10), ...] = [0.03, 0.02, ...]
```

---

#### 2.3 Reward Model è®¡ç®—å¥–åŠ±

```python
batch = reward.compute_reward(batch)
```

**Reward è¾“å‡º `token_level_scores`:**

| å­—æ®µ | å½¢çŠ¶ | ç¤ºä¾‹å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `token_level_scores` | `(batch, response_len)` | `[[0,0,0,0,0,1.0]]` | ç¨€ç–å¥–åŠ±ï¼Œä»…åœ¨EOSä½ç½®ç»™å‡ºæœ€ç»ˆå¾—åˆ† |

> å¦‚æœç­”æ¡ˆæ­£ç¡®ï¼Œæœ€ç»ˆå¾—åˆ†ä¸º1.0ï¼›é”™è¯¯ä¸º0.0

**KLæƒ©ç½šåçš„å¥–åŠ±:**
```python
token_level_rewards = token_level_scores - beta * kld
# beta=0.01 æ—¶: [[0-0.0003, 0-0.0002, ..., 1.0-0.0001]]
```

---

#### 2.4 è®¡ç®—ä¼˜åŠ¿ (Advantage)

```python
batch = compute_advantages(batch)
```

ä½¿ç”¨ **GAE (Generalized Advantage Estimation)**:

| å­—æ®µ | å½¢çŠ¶ | ç¤ºä¾‹å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `advantages` | `(batch, response_len)` | `[[0.13, 0.10, 0.08, 0.05, 0.03, 0.05]]` | ä¼°è®¡çš„ä¼˜åŠ¿å‡½æ•° A(s,a) |
| `returns` | `(batch, response_len)` | `[[0.95, 0.95, 0.96, 0.95, 0.95, 1.00]]` | æŠ˜æ‰£ç´¯ç§¯å›æŠ¥ï¼ˆç”¨äºCriticè®­ç»ƒï¼‰ |

è®¡ç®—å…¬å¼:
```python
Î´_t = r_t + Î³*V(s_{t+1}) - V(s_t)  # TDè¯¯å·®
A_t = Î£ (Î³Î»)^k * Î´_{t+k}           # GAE
```

---

### Stage 3: Actor/Critic æ›´æ–°

#### 3.1 Critic æ›´æ–°

```python
critic_metrics = critic.update_critic(batch)
```

**Critic è®­ç»ƒè¾“å‡ºï¼ˆæŒ‡æ ‡ï¼‰:**

```python
{
    'critic/vf_loss': 0.0234,        # ä»·å€¼å‡½æ•°MSEæŸå¤±
    'critic/vf_clipfrac': 0.15,      # è¢«è£å‰ªçš„æ¯”ä¾‹
    'critic/grad_norm': 0.82,        # æ¢¯åº¦èŒƒæ•°
}
```

---

#### 3.2 Actor æ›´æ–°

```python
actor_metrics = actor.update_actor(batch)
```

**Actor è®­ç»ƒè¾“å‡ºï¼ˆæŒ‡æ ‡ï¼‰:**

```python
{
    'actor/pg_loss': -0.0156,        # ç­–ç•¥æ¢¯åº¦æŸå¤±
    'actor/pg_clipfrac': 0.12,       # PPOè£å‰ªæ¯”ä¾‹
    'actor/entropy': 2.34,           # ç­–ç•¥ç†µ
    'actor/approx_kl': 0.015,        # æ–°æ—§ç­–ç•¥çš„è¿‘ä¼¼KLæ•£åº¦
    'actor/grad_norm': 1.23,         # æ¢¯åº¦èŒƒæ•°
}
```

PPOæŸå¤±è®¡ç®—:
```python
ratio = exp(new_log_prob - old_log_prob)
L_clip = min(ratio * A, clip(ratio, 1-Îµ, 1+Îµ) * A)
```

---

### å®Œæ•´æ•°æ®æµå›¾

```
Prompt: "ä½ çŸ¥é“1+1=2å—ï¼Ÿ"
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Actor.generate()                                           â”‚
â”‚  â†’ responses: "æ˜¯çš„ï¼Œ1+1=2"                                 â”‚
â”‚  â†’ old_log_probs: [-0.12, -0.08, ...]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Critic.compute_values()                                    â”‚
â”‚  â†’ values: [0.82, 0.85, 0.88, 0.90, 0.92, 0.95]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reference.compute_log_prob()                               â”‚
â”‚  â†’ ref_log_prob: [-0.15, -0.10, ...]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reward.compute_reward()                                    â”‚
â”‚  â†’ token_level_scores: [0, 0, 0, 0, 0, 1.0]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  compute_advantage() (GAE)                                  â”‚
â”‚  â†’ advantages: [0.13, 0.10, 0.08, 0.05, 0.03, 0.05]        â”‚
â”‚  â†’ returns: [0.95, 0.95, 0.96, 0.95, 0.95, 1.00]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Critic.update()       â”‚  â”‚  Actor.update()        â”‚
â”‚  loss: (values-returns)Â²â”‚  â”‚  loss: -ratio * A      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

è¿™æ ·ï¼Œæ•´ä¸ªPPOè®­ç»ƒå¾ªç¯å®Œæˆä¸€æ¬¡è¿­ä»£ã€‚


--------------

è¯·ç»“åˆray_trainer.pyä»£ç è¿›è¡Œé€æ­¥è§£é‡Šï¼ˆæ¯ä¸€æ­¥æœ€å¥½åŒ…å«æ•°å­¦å…¬å¼ã€ä»£ç ç‰‡æ®µã€å…·ä½“ç¤ºä¾‹çš„è¾“å…¥è¾“å‡ºï¼‰ã€‚

åŠ©æ‰‹ï¼šå¥½çš„ï¼Œæˆ‘å°†ç»“åˆ `ray_trainer.py` çš„ä»£ç å®ç°ï¼Œä¸²è”æ•´ä¸ª PPO è®­ç»ƒæ•°æ®æµã€‚

---

# PPO è®­ç»ƒæµç¨‹è¯¦è§£ï¼ˆåŸºäº RayPPOTrainer æºç ï¼‰

## Stage 1: Actor ç”Ÿæˆå“åº” (Evolution)

### 1.1 è§¦å‘ç”Ÿæˆ
åœ¨ `fit()` æ–¹æ³•ä¸­ï¼ˆçº¦ 710-730 è¡Œï¼‰ï¼Œé¦–å…ˆä» `dataloader` è·å– Prompt å¹¶è°ƒç”¨ `ActorRollout` äº§ç”Ÿå¯¹è¯ç»“æœã€‚

```python
# ray_trainer.py: L711
gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)
```

### 1.2 æ•°æ®ç»†èŠ‚ (ç¤ºä¾‹)
- **Input (Prompt):** "ä½ çŸ¥é“1+1=2å—ï¼Ÿ"
- **Actor è¾“å‡º `DataProto`:**
    - `responses`: `[[100, 101, 102...]]` (è§£ç ä¸º "æ˜¯çš„ï¼Œ1+1=2")
    - `old_log_probs`: é‡‡æ ·è¿™äº› token æ—¶çš„ log æ¦‚ç‡æ¦‚ç‡ã€‚
    - `attention_mask`: æ ‡è®°éå¡«å……ä½ç½®ã€‚

---

## Stage 2: è®­ç»ƒæ•°æ®å‡†å¤‡ (Experience Preparation)

ç”Ÿæˆå›ç­”åï¼Œéœ€è¦å‡‘é½è®¡ç®— Advantage æ‰€æœ‰çš„â€œæ‹¼å›¾â€ã€‚

### 2.1 Reference Policy è®¡ç®— KL åŸºå‡† (çº¦ 766 è¡Œ)
Reference æ¨¡å‹ï¼ˆFrozenï¼‰è®¡ç®—ç›¸åŒè¾“å‡ºåœ¨åŸç­–ç•¥ä¸‹çš„ `ref_log_prob`ï¼Œç”¨äºé˜²æ­¢è®­ç»ƒåçš„ç­–ç•¥è¿‡å¿«åç¦»ã€‚

$$D_{KL} \approx \log \pi_{\theta}(a|s) - \log \pi_{ref}(a|s)$$

### 2.2 Critic è®¡ç®—ä»·å€¼é¢„æµ‹ (çº¦ 772 è¡Œ)
Critic ç½‘ç»œç»™å‡ºæ¯ä¸ªæ—¶é—´æ­¥çš„ $V(s_t)$ã€‚
- **Output `values`:** `[[0.82, 0.85, ...]]`

### 2.3 Reward Model è®¡ç®—å¾—åˆ† (çº¦ 778-789 è¡Œ)
è°ƒç”¨å¥–åŠ±å‡½æ•°ï¼ˆå¯èƒ½æ˜¯å›å½’æ¨¡å‹æˆ–è§„åˆ™å¼•æ“ï¼‰ã€‚
- **Output `token_level_scores`:** `[[0, 0, 0, 0, 1.0]]` (ç¨€ç–å¥–åŠ±)

### 2.4 KL æƒ©ç½šä¸ç»„åˆå›æŠ¥ (çº¦ 791 è¡Œ)
é€šè¿‡ `apply_kl_penalty` å‡½æ•°å°†å¥–åŠ±å’Œ KL æƒ©ç½šç»“åˆï¼š
$$R_t = \text{score}_t - \beta \cdot (\log \pi_{\theta} - \log \pi_{ref})$$

### 2.5 è®¡ç®—ä¼˜åŠ¿ (Advantage) - GAEç®—æ³• (çº¦ 800 è¡Œ)
è°ƒç”¨ `compute_advantage` ä½¿ç”¨ GAEã€‚
- **å…¬å¼:** $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$
- **Advantage:** $A_t = \sum (\gamma\lambda)^k \delta_{t+k}$
- **Returns:** $R_t = A_t + V(s_t)$

---

## Stage 3: æ¨¡å‹æ›´æ–° (Optimization)

### 3.1 Critic ç½‘ç»œæ›´æ–° (çº¦ 807-812 è¡Œ)
é€šè¿‡ MSE Loss æœ€å°åŒ–ä»·å€¼ä¼°è®¡è¯¯å·®ã€‚
$$Loss_{Critic} = \frac{1}{2} (V_{\phi}(s) - \text{Returns})^2$$

### 3.2 Actor ç½‘ç»œæ›´æ–° (çº¦ 814-822 è¡Œ)
ä½¿ç”¨è£å‰ªåçš„æ¯”ç‡è¿›è¡Œç­–ç•¥æ¢¯åº¦æ›´æ–°ã€‚

$$ratio = \exp(\log \pi_{new} - \log \pi_{old})$$
$$Loss_{Actor} = -\min(ratio \cdot A_t, \text{clip}(ratio, 1-\epsilon, 1+\epsilon) \cdot A_t)$$

---

## å®Œæ•´æ‰§è¡Œæ‹“æ‰‘å›¾

é©±åŠ¨å™¨ (Driver) æŒ‡æŒ¥å„ Ray Worker ååŒï¼š
1. **Rollout Workers:** æ¨ç†ç”Ÿæˆ
2. **Ref Workers:** è®¡ç®—æ—§æ¦‚ç‡
3. **Reward Workers:** è¯„åˆ†
4. **Critic Workers:** ä¼°å€¼ä¸è®­ç»ƒ
5. **Actor Workers:** ç­–ç•¥ä¼˜åŒ–

æ•´ä¸ªå¾ªç¯ä¸æ–­è¿­ä»£ï¼Œç›´åˆ°æ»¡è¶³ `total_training_steps`ã€‚

---

### é—®é¢˜ 2
ç”¨æˆ·ï¼šrole_worker_mapping = {
        Role.ActorRollout: ray.remote(ActorRolloutRefWorker),  # Actor + Rollout
        Role.Critic: ray.remote(CriticWorker),                 # Critic
        Role.RefPolicy: ray.remote(ActorRolloutRefWorker),     # Reference Policy
    }

  ------

  è¿›è¡ŒRLè®­ç»ƒçš„æ—¶å€™ï¼ŒActorRolloutæ˜¯ä¸€ä¸ªæ¨¡å‹è¿˜æ˜¯ä¸¤ä¸ªæ¨¡å‹ï¼Ÿè¿›è¡Œrolloutçš„æ¨¡å‹æ˜¯æœ€ç»ˆè¢«è®­ç»ƒçš„æ¨¡å‹ä¹ˆï¼Ÿ

åŠ©æ‰‹ï¼šè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼è®©æˆ‘æŸ¥çœ‹ç›¸å…³ä»£ç æ¥å‡†ç¡®å›ç­”ï¼š

## å›ç­”ï¼šActorRollout æ˜¯ä¸€ä¸ªæ¨¡å‹è¿˜æ˜¯ä¸¤ä¸ªæ¨¡å‹ï¼Ÿ

### æ ¸å¿ƒç­”æ¡ˆï¼š**ä¸€ä¸ªæ¨¡å‹ï¼Œå¤šç§è§’è‰²**

ä»ä»£ç ä¸­å¯ä»¥æ¸…æ¥šçœ‹åˆ°ï¼ˆ`fsdp_workers.py` ç¬¬47-82è¡Œï¼‰ï¼š

```python
class ActorRolloutRefWorker(Worker):
    """
    This worker can be instantiated as a standalone actor or a standalone rollout or a standalone reference policy
    or a hybrid engine based on the config.rollout
    """

    def __init__(self, config: DictConfig, role: str):
        # ...
        self.role = role
        assert self.role in ['actor', 'rollout', 'ref', 'actor_rollout', 'actor_rollout_ref']

        self._is_actor = self.role in ['actor', 'actor_rollout', 'actor_rollout_ref']
        self._is_rollout = self.role in ['rollout', 'actor_rollout', 'actor_rollout_ref']
        self._is_ref = self.role in ['ref', 'actor_rollout_ref']
```

---

### å…³é”®ç†è§£

| è§’è‰²é…ç½® | å®é™…æ¨¡å‹ | è¯´æ˜ |
|---------|---------|------|
| `Role.ActorRollout` (role='actor_rollout') | **1ä¸ªæ¨¡å‹** | Actor + Rollout å…±ç”¨åŒä¸€ä¸ªæ¨¡å‹ |
| `Role.RefPolicy` (role='ref') | **1ä¸ªç‹¬ç«‹æ¨¡å‹** | Reference Policy æ˜¯å•ç‹¬åŠ è½½çš„æ¨¡å‹ |
| `Role.Critic` | **1ä¸ªç‹¬ç«‹æ¨¡å‹** | Critic æ˜¯ç‹¬ç«‹çš„ä»·å€¼ç½‘ç»œ |

ä» `ray_trainer.py` ç¬¬559-584è¡Œå¯ä»¥çœ‹åˆ°åˆå§‹åŒ–è¿‡ç¨‹ï¼š

```python
# ActorRollout ä½¿ç”¨ 'actor_rollout' role
actor_rollout_cls = RayClassWithInitArgs(cls=self.role_worker_mapping[Role.ActorRollout],
                                         config=self.config.actor_rollout_ref,
                                         role='actor_rollout')  # â† åŒä¸€ä¸ªæ¨¡å‹åšä¸¤ä»¶äº‹

# RefPolicy ä½¿ç”¨ 'ref' role - è¿™æ˜¯ç‹¬ç«‹çš„å¦ä¸€ä¸ªæ¨¡å‹
ref_policy_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RefPolicy],
                                      config=self.config.actor_rollout_ref,
                                      role='ref')  # â† ç‹¬ç«‹åŠ è½½çš„å‚è€ƒç­–ç•¥æ¨¡å‹
```

---

### å›¾è§£ï¼šæ¨¡å‹å®ä¾‹å…³ç³»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PPO Training ä¸­çš„æ¨¡å‹                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚      ActorRolloutRefWorker (role='actor_rollout')           â”‚       â”‚
â”‚   â”‚      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â”‚       â”‚
â”‚   â”‚                                                             â”‚       â”‚
â”‚   â”‚   self.actor_module_fsdp â† åªæœ‰ä¸€ä¸ªæ¨¡å‹å®ä¾‹ï¼               â”‚       â”‚
â”‚   â”‚              â”‚                                              â”‚       â”‚
â”‚   â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                       â”‚       â”‚
â”‚   â”‚       â–¼             â–¼                                       â”‚       â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚       â”‚
â”‚   â”‚   â”‚ Actor â”‚    â”‚ Rollout  â”‚  â† ä¸¤ä¸ªè§’è‰²ï¼Œå…±äº«åŒä¸€ä¸ªæ¨¡å‹     â”‚       â”‚
â”‚   â”‚   â”‚       â”‚    â”‚ (ç”Ÿæˆ)   â”‚                                 â”‚       â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚       â”‚
â”‚   â”‚                                                             â”‚       â”‚
â”‚   â”‚   åŠŸèƒ½:                                                     â”‚       â”‚
â”‚   â”‚   - generate_sequences(): ç”¨äºç”Ÿæˆå“åº” (Rollout)            â”‚       â”‚
â”‚   â”‚   - update_actor(): ç”¨äºç­–ç•¥æ¢¯åº¦æ›´æ–° (Actor)                â”‚       â”‚
â”‚   â”‚                                                             â”‚       â”‚
â”‚   â”‚   âœ… ä¼šè¢«è®­ç»ƒå’Œæ›´æ–°ï¼                                        â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚      ActorRolloutRefWorker (role='ref')                     â”‚       â”‚
â”‚   â”‚      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                   â”‚       â”‚
â”‚   â”‚                                                             â”‚       â”‚
â”‚   â”‚   self.ref_module_fsdp â† ç‹¬ç«‹åŠ è½½çš„æ¨¡å‹ï¼ˆå†»ç»“ï¼‰             â”‚       â”‚
â”‚   â”‚              â”‚                                              â”‚       â”‚
â”‚   â”‚              â–¼                                              â”‚       â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚       â”‚
â”‚   â”‚   â”‚ Reference Policyâ”‚ â† ç”¨äºè®¡ç®— KL æ•£åº¦çš„å‚è€ƒç­–ç•¥          â”‚       â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚       â”‚
â”‚   â”‚                                                             â”‚       â”‚
â”‚   â”‚   åŠŸèƒ½:                                                     â”‚       â”‚
â”‚   â”‚   - compute_ref_log_prob(): è®¡ç®—å‚è€ƒç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡          â”‚       â”‚
â”‚   â”‚                                                             â”‚       â”‚
â”‚   â”‚   âŒ ä¸ä¼šè¢«è®­ç»ƒï¼ä¿æŒå†»ç»“çŠ¶æ€                                â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚      CriticWorker                                           â”‚       â”‚
â”‚   â”‚      â•â•â•â•â•â•â•â•â•â•â•â•                                           â”‚       â”‚
â”‚   â”‚                                                             â”‚       â”‚
â”‚   â”‚   self.critic_module â† ç‹¬ç«‹çš„ä»·å€¼ç½‘ç»œ                       â”‚       â”‚
â”‚   â”‚                                                             â”‚       â”‚
â”‚   â”‚   åŠŸèƒ½:                                                     â”‚       â”‚
â”‚   â”‚   - compute_values(): ä¼°è®¡çŠ¶æ€ä»·å€¼                          â”‚       â”‚
â”‚   â”‚   - update_critic(): æ›´æ–°ä»·å€¼ç½‘ç»œ                           â”‚       â”‚
â”‚   â”‚                                                             â”‚       â”‚
â”‚   â”‚   âœ… ä¼šè¢«è®­ç»ƒå’Œæ›´æ–°ï¼                                        â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### å›ç­”ç¬¬äºŒä¸ªé—®é¢˜ï¼šè¿›è¡Œ Rollout çš„æ¨¡å‹æ˜¯æœ€ç»ˆè¢«è®­ç»ƒçš„æ¨¡å‹å—ï¼Ÿ

**æ˜¯çš„ï¼Œå®Œå…¨æ­£ç¡®ï¼** 

ä»ä»£ç å¯ä»¥çœ‹åˆ°ï¼ˆ`fsdp_workers.py` ç¬¬295-332è¡Œï¼‰ï¼š

```python
if self._is_actor or self._is_rollout:
    # we need the model for actor and rollout
    # ...
    self.actor_module_fsdp, self.actor_optimizer, self.actor_lr_scheduler, self.actor_model_config = self._build_model_optimizer(...)
    
    # get the original unwrapped module
    self.actor_module = self.actor_module_fsdp._fsdp_wrapped_module  # â† åŒä¸€ä¸ªåº•å±‚æ¨¡å‹

if self._is_actor:
    self.actor = DataParallelPPOActor(config=self.config.actor,
                                      actor_module=self.actor_module_fsdp,  # â† Actor ç”¨è¿™ä¸ªæ¨¡å‹
                                      actor_optimizer=self.actor_optimizer)

if self._is_rollout:
    self.rollout, self.rollout_sharding_manager = self._build_rollout()  # â† Rollout ä¹Ÿç”¨è¿™ä¸ªæ¨¡å‹
```

**è®­ç»ƒæµç¨‹ä¸­çš„æ¨¡å‹ä½¿ç”¨ï¼š**

```
1. Rollout (ç”Ÿæˆ):
   gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)
                                    â†“
                      ä½¿ç”¨ self.actor_module_fsdp ç”Ÿæˆå“åº”
                      
2. Actor (æ›´æ–°):
   actor_output = self.actor_rollout_wg.update_actor(batch)
                                    â†“
                      æ›´æ–°åŒä¸€ä¸ª self.actor_module_fsdp çš„å‚æ•°
```

---

### æ€»ç»“

| é—®é¢˜ | ç­”æ¡ˆ |
|------|------|
| ActorRollout æ˜¯ä¸€ä¸ªæ¨¡å‹è¿˜æ˜¯ä¸¤ä¸ªï¼Ÿ | **ä¸€ä¸ªæ¨¡å‹**ï¼Œå…¼å…· Actorï¼ˆç­–ç•¥ä¼˜åŒ–ï¼‰å’Œ Rolloutï¼ˆç”Ÿæˆï¼‰ä¸¤ä¸ªè§’è‰² |
| Rollout æ¨¡å‹æ˜¯è¢«è®­ç»ƒçš„æ¨¡å‹å—ï¼Ÿ | **æ˜¯çš„**ï¼ŒRollout å’Œ Actor å…±äº«åŒä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢« PPO ç®—æ³•æ›´æ–° |
| Reference Policy å‘¢ï¼Ÿ | **ç‹¬ç«‹çš„æ¨¡å‹**ï¼Œä»ç›¸åŒè·¯å¾„åŠ è½½ä½†ä¿æŒå†»ç»“ï¼Œç”¨äºè®¡ç®— KL æ•£åº¦ |

è¿™ç§è®¾è®¡çš„åŸå› æ˜¯ï¼š
1. **æ•ˆç‡**ï¼šé¿å…ç»´æŠ¤ä¸¤ä¸ªç›¸åŒç»“æ„çš„æ¨¡å‹å‰¯æœ¬ï¼ŒèŠ‚çœæ˜¾å­˜
2. **ä¸€è‡´æ€§**ï¼šç¡®ä¿ç”Ÿæˆæ—¶ä½¿ç”¨çš„ç­–ç•¥å°±æ˜¯è¢«ä¼˜åŒ–çš„ç­–ç•¥
3. **Hybrid Engine**ï¼švLLM ç­‰æ¨ç†å¼•æ“å¯ä»¥å’Œè®­ç»ƒæ¡†æ¶å…±äº«æƒé‡

---

### é—®é¢˜ 3
ç”¨æˆ·ï¼š```
Q2: Rollout ä½¿ç”¨çš„æ¨¡å‹æ˜¯æœ€ç»ˆè¢«è®­ç»ƒçš„æ¨¡å‹å—ï¼Ÿ

  ç­”æ¡ˆï¼šæ˜¯çš„ï¼Œå®ƒä»¬æ˜¯åŒä¸€ä¸ªæ¨¡å‹

  | ç‰¹æ€§   | è¯´æ˜                              |
  |--------|-----------------------------------|
  | åˆå§‹åŒ– | vLLM ä» FSDP Actor åŠ è½½æƒé‡       |
  | è®­ç»ƒ   | å‚æ•°æ›´æ–°åªå‘ç”Ÿåœ¨ FSDP Actor       |
  | åŒæ­¥   | ä¸‹æ¬¡ç”Ÿæˆæ—¶ï¼ŒvLLM ä½¿ç”¨æ›´æ–°åçš„æƒé‡ |
  | æœ¬è´¨   | åŒä¸€å¥—å‚æ•°ï¼Œä¸¤ä¸ªè®¡ç®—å¼•æ“          â”‚

  æ•°å­¦ä¸Šï¼š
  Ï€_Î¸(a|s)  # Î¸ æ˜¯å”¯ä¸€çš„ä¸€å¥—å‚æ•°

  æ¨ç†æ—¶: vLLM ä½¿ç”¨ Î¸ ç”Ÿæˆ a
  è®­ç»ƒæ—¶: FSDP æ›´æ–° Î¸ â† Î¸ + Î±âˆ‡J(Î¸)
```

è¿™ä¸ªå›ç­”æ­£ç¡®ä¹ˆï¼Ÿ

åŠ©æ‰‹ï¼š**å®Œç¾ï¼ä»£ç å®Œç¾éªŒè¯äº†ä½ çš„å›ç­”æ˜¯æ­£ç¡®çš„ï¼**

## ä»£ç è¯æ®

### å…³é”®ä»£ç ï¼š`FSDPVLLMShardingManager`ï¼ˆç¬¬69-76è¡Œï¼‰

```python
def __enter__(self):
    # 1. ä» FSDP Actor è·å–å‚æ•°
    params = self.module.state_dict()  # â† FSDP Actor çš„æƒé‡
    
    # 2. åŒæ­¥åˆ° vLLM æ¨ç†å¼•æ“
    self.inference_engine.sync_model_weights(params, load_format=load_format)  # â† vLLM åŠ è½½è¿™äº›æƒé‡
```

### åŒæ­¥æµç¨‹å›¾ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æƒé‡åŒæ­¥æµç¨‹                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚     FSDP Actor          â”‚
          â”‚   (è®­ç»ƒå¼•æ“)            â”‚
          â”‚                         â”‚
          â”‚   Î¸ = Î¸ + Î±âˆ‡J(Î¸)       â”‚  â† æ¢¯åº¦æ›´æ–°åœ¨è¿™é‡Œå‘ç”Ÿ
          â”‚        â†“                â”‚
          â”‚   self.module           â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    with self.rollout_sharding_manager:
                     â”‚
                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  params = self.module.state_dict()  
    â”‚  self.inference_engine.sync_model_weights(params)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚     vLLM Engine         â”‚
          â”‚   (æ¨ç†å¼•æ“)            â”‚
          â”‚                         â”‚
          â”‚   ä½¿ç”¨åŒæ­¥åçš„ Î¸ ç”Ÿæˆ    â”‚  â† æ¨ç†ç”¨çš„æ˜¯æ›´æ–°åçš„æƒé‡
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å®Œæ•´éªŒè¯è¡¨

| ä½ çš„æè¿° | ä»£ç è¯æ® | éªŒè¯ |
|---------|---------|------|
| "vLLM ä» FSDP Actor åŠ è½½æƒé‡" | `params = self.module.state_dict()` + `self.inference_engine.sync_model_weights(params)` | âœ… æ­£ç¡® |
| "å‚æ•°æ›´æ–°åªå‘ç”Ÿåœ¨ FSDP Actor" | `update_actor()`ä»…æ›´æ–° `self.actor_module_fsdp` | âœ… æ­£ç¡® |
| "ä¸‹æ¬¡ç”Ÿæˆæ—¶ï¼ŒvLLM ä½¿ç”¨æ›´æ–°åçš„æƒé‡" | æ¯æ¬¡ `generate_sequences()` éƒ½é€šè¿‡ `with self.rollout_sharding_manager:` åŒæ­¥ | âœ… æ­£ç¡® |
| "åŒä¸€å¥—å‚æ•°ï¼Œä¸¤ä¸ªè®¡ç®—å¼•æ“" | FSDPç”¨äºè®­ç»ƒï¼ŒvLLMç”¨äºé«˜æ•ˆæ¨ç†ï¼Œå…±äº«Î¸ | âœ… æ­£ç¡® |

---

## è¡¥å……ç»†èŠ‚

ä» `generate_sequences()` è°ƒç”¨æµç¨‹æ¥çœ‹ï¼ˆ`fsdp_workers.py` ç¬¬446è¡Œï¼‰ï¼š

```python
@register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)
def generate_sequences(self, prompts: DataProto):
    # ...
    with self.rollout_sharding_manager:  # â† è¿›å…¥æ—¶åŒæ­¥æƒé‡ï¼
        prompts = self.rollout_sharding_manager.preprocess_data(prompts)
        output = self.rollout.generate_sequences(prompts=prompts)  # â† vLLM ä½¿ç”¨åŒæ­¥åçš„æƒé‡
        output = self.rollout_sharding_manager.postprocess_data(output)
```

**ä½ çš„å›ç­”å®Œå…¨æ­£ç¡®**ï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾ç‚¼ä¸”å‡†ç¡®çš„æ€»ç»“ ğŸ‘
