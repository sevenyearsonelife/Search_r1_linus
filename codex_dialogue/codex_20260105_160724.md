# Chat Dialogue Log

## å¯¹è¯æ—¶é—´
2026-01-05 16:07:24

---

## å¯¹è¯å†…å®¹

### é—®é¢˜ 1
ç”¨æˆ·ï¼šè¿™ä»½è„šæœ¬çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿå®ç°äº†training loopä¹ˆï¼Ÿ
åŠ©æ‰‹ï¼š## è„šæœ¬åŠŸèƒ½æ¦‚è¿°

è¿™ä»½ `ray_trainer.py` è„šæœ¬æ˜¯ä¸€ä¸ª**åŸºäº Ray åˆ†å¸ƒå¼æ¡†æ¶çš„ FSDP PPO (Proximal Policy Optimization) Trainer**ï¼Œæ˜¯ VERL (Reinforcement Learning) æ¡†æ¶çš„æ ¸å¿ƒè®­ç»ƒå™¨ã€‚

### ä¸»è¦åŠŸèƒ½

1. **åˆ†å¸ƒå¼ PPO è®­ç»ƒæ§åˆ¶å™¨** - ä½¿ç”¨ Ray çš„å•æ§åˆ¶å™¨æ¶æ„æ¥åè°ƒå¤šä¸ª Workerï¼ˆActorã€Criticã€Rolloutã€RefPolicyã€RewardModelï¼‰

2. **èµ„æºæ± ç®¡ç†** - é€šè¿‡ `ResourcePoolManager` ç±»ç®¡ç†ä¸åŒè§’è‰²çš„ GPU èµ„æºåˆ†é…

3. **æ··åˆå¼•æ“æ”¯æŒ** - æ”¯æŒ Actor + Rollout æ··åˆæ¨¡å¼ï¼ˆ`hybrid_engine`ï¼‰

---

### âœ… æ˜¯çš„ï¼Œå®ƒå®ç°äº†å®Œæ•´çš„ Training Loop

æ ¸å¿ƒè®­ç»ƒå¾ªç¯åœ¨ **`fit()` æ–¹æ³•**ï¼ˆç¬¬ 654-852 è¡Œï¼‰ä¸­ï¼š

```python
for epoch in range(self.config.trainer.total_epochs):        # å¤–å±‚ epoch å¾ªç¯
    for batch_dict in self.train_dataloader:                  # å†…å±‚ batch å¾ªç¯
        # 1. ç”Ÿæˆåºåˆ— (Rollout)
        # 2. è®¡ç®— reference log_prob
        # 3. è®¡ç®— values (Critic)
        # 4. è®¡ç®— rewards (Reward Model/Function)
        # 5. åº”ç”¨ KL penalty
        # 6. è®¡ç®— advantages (GAE æˆ– GRPO)
        # 7. æ›´æ–° Critic
        # 8. æ›´æ–° Actor (PPO)
        # 9. éªŒè¯ & Checkpoint ä¿å­˜
```

### å…³é”®è®­ç»ƒæ­¥éª¤

| æ­¥éª¤ | è¡Œå· | æè¿° |
|------|------|------|
| ç”Ÿæˆ | 712-748 | ä½¿ç”¨ `actor_rollout_wg.generate_sequences()` æˆ– Agent å¾ªç¯ç”Ÿæˆå“åº” |
| å‚è€ƒç­–ç•¥ | 768-770 | è®¡ç®— `ref_log_prob` ç”¨äº KL æƒ©ç½š |
| ä»·å€¼ä¼°è®¡ | 774-776 | `critic_wg.compute_values()` |
| å¥–åŠ±è®¡ç®— | 782-789 | ç»“åˆæ¨¡å‹å’Œè§„åˆ™å¥–åŠ±å‡½æ•° |
| ä¼˜åŠ¿ä¼°è®¡ | 801-805 | GAE æˆ– GRPO ç®—æ³• |
| Critic æ›´æ–° | 809-812 | `critic_wg.update_critic()` |
| Actor æ›´æ–° | 817-822 | `actor_rollout_wg.update_actor()` |

è¿™æ˜¯ä¸€ä¸ªç”Ÿäº§çº§çš„å®Œæ•´ PPO è®­ç»ƒå®ç°ï¼Œæ”¯æŒæœç´¢å¢å¼ºçš„ Agent æ¨¡å¼ï¼ˆ`do_search`ï¼‰å’Œæ ‡å‡† RL æ¨¡å¼ã€‚

---

### é—®é¢˜ 2
ç”¨æˆ·ï¼šä½¿ç”¨ Ray çš„å•æ§åˆ¶å™¨æ¶æ„æ¥åè°ƒå¤šä¸ª Workerï¼ˆActorã€Criticã€Rolloutã€RefPolicyã€RewardModelï¼‰ã€‚è¯·ç»“åˆå…·ä½“çš„ç¤ºä¾‹è§£é‡Šåè°ƒæœºåˆ¶ã€‚
åŠ©æ‰‹ï¼šç°åœ¨æˆ‘æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥è¯¦ç»†è§£é‡Š Ray å•æ§åˆ¶å™¨æ¶æ„çš„åè°ƒæœºåˆ¶äº†ã€‚

## Ray å•æ§åˆ¶å™¨æ¶æ„åè°ƒæœºåˆ¶è¯¦è§£

### æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Driver Process (CPU)                        â”‚
â”‚                      RayPPOTrainer.fit()                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              ResourcePoolManager                          â”‚  â”‚
â”‚  â”‚   ç®¡ç† GPU èµ„æºæ± å¹¶å°† Role æ˜ å°„åˆ°å¯¹åº”èµ„æºæ±                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚              â–¼                               â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   RayWorkerGroup    â”‚         â”‚   RayWorkerGroup    â”‚       â”‚
â”‚  â”‚  (actor_rollout_wg) â”‚         â”‚    (critic_wg)      â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ RPC è°ƒç”¨                        â”‚ RPC è°ƒç”¨
              â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU Worker 0          â”‚     â”‚   GPU Worker 1          â”‚
â”‚   (Ray Actor)           â”‚     â”‚   (Ray Actor)           â”‚
â”‚   - Actor æ¨¡å‹          â”‚     â”‚   - Critic æ¨¡å‹          â”‚
â”‚   - Rollout å¼•æ“        â”‚     â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### 1. **ResourcePoolManager** - èµ„æºæ± ç®¡ç†

```python
@dataclass
class ResourcePoolManager:
    resource_pool_spec: dict[str, list[int]]  # ä¾‹: {"pool1": [8], "pool2": [4]}
    mapping: dict[Role, str]                   # ä¾‹: {Role.Actor: "pool1", Role.Critic: "pool2"}
```

**å…·ä½“ç¤ºä¾‹**ï¼šå‡è®¾æœ‰ 2 ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹ 8 å¼  GPU

```python
resource_pool_spec = {
    "actor_rollout_pool": [8],  # èŠ‚ç‚¹0: 8ä¸ªè¿›ç¨‹
    "critic_pool": [4]          # èŠ‚ç‚¹1: 4ä¸ªè¿›ç¨‹
}

mapping = {
    Role.ActorRollout: "actor_rollout_pool",
    Role.Critic: "critic_pool"
}
```

#### 2. **RayWorkerGroup** - Worker ç»„ç®¡ç†

`RayWorkerGroup` æ˜¯å•æ§åˆ¶å™¨ä¸å¤šä¸ª Ray Actor Workers ä¹‹é—´çš„æ¡¥æ¢ï¼š

```python
class RayWorkerGroup(WorkerGroup):
    def __init__(self, resource_pool, ray_cls_with_init, ...):
        # æ ¹æ®èµ„æºæ± åˆ›å»ºå¤šä¸ª Ray Actor
        self._init_with_resource_pool(resource_pool, ray_cls_with_init, ...)
```

**å…³é”®æ–¹æ³•**:
- `execute_all_sync()` - åŒæ­¥æ‰§è¡Œæ‰€æœ‰ worker çš„æ–¹æ³•
- `execute_all_async()` - å¼‚æ­¥æ‰§è¡Œ
- `execute_rank_zero_sync()` - åªåœ¨ rank 0 æ‰§è¡Œ

#### 3. **create_colocated_worker_cls** - å…±äº«è¿›ç¨‹æœºåˆ¶

```python
def create_colocated_worker_cls(class_dict: dict[str, RayClassWithInitArgs]):
    """
    å°†å¤šä¸ªè§’è‰²ï¼ˆå¦‚ Actor + Rolloutï¼‰åˆå¹¶åˆ°åŒä¸€ä¸ªè¿›ç¨‹ä¸­
    èŠ‚çœ GPU å†…å­˜å’Œè¿›ç¨‹é—´é€šä¿¡å¼€é”€
    """
    class WorkerDict(worker_cls):
        def __init__(self):
            self.worker_dict = {}
            for key, cls in cls_dict.items():
                self.worker_dict[key] = cls(...)  # åœ¨åŒä¸€è¿›ç¨‹ä¸­å®ä¾‹åŒ–å¤šä¸ªæ¨¡å‹
```

---

### è®­ç»ƒå¾ªç¯ä¸­çš„åè°ƒç¤ºä¾‹

ä»¥ `fit()` ä¸­çš„ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸ºä¾‹ï¼š

```python
# è®­ç»ƒå¾ªç¯ä¸­çš„åè°ƒæµç¨‹
with _timer('step', timing_raw):
    
    # â‘  ç”Ÿæˆåºåˆ— - Actor/Rollout Worker
    gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)
    #   Driver â†’ åˆ†å‘æ•°æ®åˆ°æ‰€æœ‰ worker â†’ å¹¶è¡Œç”Ÿæˆ â†’ æ”¶é›†ç»“æœ
    
    # â‘¡ è®¡ç®—å‚è€ƒç­–ç•¥çš„ log_prob - RefPolicy Worker  
    if self.use_reference_policy:
        ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)
        #   Driver â†’ RefPolicy Workers â†’ è¿”å› log_prob
    
    # â‘¢ è®¡ç®—ä»·å€¼å‡½æ•° - Critic Worker
    if self.use_critic:
        values = self.critic_wg.compute_values(batch)
        #   Driver â†’ Critic Workers â†’ è¿”å› values
    
    # â‘£ è®¡ç®—å¥–åŠ± - å¯èƒ½ä½¿ç”¨ RewardModel Worker
    if self.use_rm:
        reward_tensor = self.rm_wg.compute_rm_score(batch)
    
    # â‘¤ è½»é‡çº§è®¡ç®—åœ¨ Driver ä¸Šæ‰§è¡Œ
    batch = compute_advantage(batch, ...)  # ä¼˜åŠ¿ä¼°è®¡
    batch, kl_metrics = apply_kl_penalty(batch, ...)  # KL æƒ©ç½š
    
    # â‘¥ æ›´æ–°æ¨¡å‹å‚æ•° - åˆ†å¸ƒå¼æ¢¯åº¦æ›´æ–°
    critic_output = self.critic_wg.update_critic(batch)  # Critic æ›´æ–°
    actor_output = self.actor_rollout_wg.update_actor(batch)  # Actor æ›´æ–°
```

---

### æ ¸å¿ƒåè°ƒæœºåˆ¶ï¼šDispatch & Execute

Worker æ–¹æ³•é€šè¿‡è£…é¥°å™¨æ³¨å†Œåˆ° WorkerGroup:

```python
# åœ¨ Worker ç±»ä¸­
@register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)  # æ•°æ®å¹¶è¡Œåˆ†å‘
def compute_values(self, data: DataProto):
    # æ¯ä¸ª worker å¤„ç†ä¸€éƒ¨åˆ†æ•°æ®
    return values
```

**WorkerGroup è‡ªåŠ¨å¤„ç†**:
1. **Dispatch (åˆ†å‘)**: å°† `DataProto` æŒ‰ `world_size` åˆ†å‰²ï¼Œå‘é€åˆ°å„ worker
2. **Execute (æ‰§è¡Œ)**: å„ worker å¹¶è¡Œè®¡ç®—
3. **Collect (æ”¶é›†)**: æ”¶é›†å„ worker ç»“æœï¼Œåˆå¹¶è¿”å›ç»™ Driver

```python
# WorkerGroup._bind_worker_method ä¸­çš„é€»è¾‘
def _bind_worker_method(self, user_defined_cls, func_generator):
    for method_name in dir(user_defined_cls):
        if hasattr(method, MAGIC_ATTR):
            dispatch_fn = get_predefined_dispatch_fn(dispatch_mode)['dispatch_fn']
            collect_fn = get_predefined_dispatch_fn(dispatch_mode)['collect_fn']
            
            # ç”ŸæˆåŒ…è£…å‡½æ•°ï¼šdispatch â†’ execute â†’ collect
            func = func_generator(self, method_name, 
                                  dispatch_fn=dispatch_fn,
                                  collect_fn=collect_fn, 
                                  execute_fn=execute_fn)
            setattr(self, method_name, func)
```

---

### æ•°æ®æµç¤ºæ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Driver                                 â”‚
â”‚                                                                â”‚
â”‚  batch = DataProto(...)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                         dispatch_fn åˆ†å‰²                    â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                         â”‚                                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
         â–¼               â–¼               â–¼                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚ Worker0 â”‚     â”‚ Worker1 â”‚     â”‚ Worker2 â”‚                 â”‚
    â”‚ batch/3 â”‚     â”‚ batch/3 â”‚     â”‚ batch/3 â”‚                 â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                 â”‚
         â”‚               â”‚               â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
                         â–¼                                       â”‚
                    collect_fn åˆå¹¶                               â”‚
                         â”‚                                       â”‚
                         â–¼                                       â”‚
                  è¿”å›å®Œæ•´ç»“æœç»™ Driver â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### æ€»ç»“

| ç»„ä»¶ | èŒè´£ |
|------|------|
| **Driver** | å•æ§åˆ¶å™¨ï¼Œè¿è¡Œ `RayPPOTrainer.fit()`ï¼Œåè°ƒæ•°æ®æµå’Œè°ƒåº¦ |
| **ResourcePoolManager** | ç®¡ç† GPU èµ„æºï¼Œå°†è§’è‰²æ˜ å°„åˆ°èµ„æºæ±  |
| **RayWorkerGroup** | å°è£…ä¸€ç»„ Ray Actorï¼Œæä¾›ç»Ÿä¸€çš„ RPC è°ƒç”¨æ¥å£ |
| **create_colocated_worker_cls** | å…è®¸å¤šä¸ªè§’è‰²å…±äº«åŒä¸€ GPU è¿›ç¨‹ |
| **Dispatch/Collect** | è‡ªåŠ¨æ•°æ®åˆ†å‘å’Œç»“æœæ”¶é›† |

è¿™ç§æ¶æ„çš„ä¼˜åŠ¿åœ¨äº **Driver ä¸æŒæœ‰ GPU èµ„æº**ï¼Œæ‰€æœ‰è®¡ç®—éƒ½åœ¨ Worker ä¸Šå®Œæˆï¼ŒDriver åªè´Ÿè´£è½»é‡çº§çš„æ•°æ®è°ƒåº¦å’Œä¼˜åŠ¿è®¡ç®—ã€‚

---

### é—®é¢˜ 3
ç”¨æˆ·ï¼šåœ¨è®­ç»ƒå¾ªç¯ä¸­actorå’Œcriticæ˜¯ä¸æ˜¯éƒ½è¦è¿›è¡Œæ¨¡å‹æ›´æ–°ï¼Ÿ
åŠ©æ‰‹ï¼šæ˜¯çš„ï¼Œåœ¨æ ‡å‡† PPO è®­ç»ƒå¾ªç¯ä¸­ï¼Œ**Actor å’Œ Critic éƒ½éœ€è¦è¿›è¡Œæ¨¡å‹æ›´æ–°**ã€‚ä½†å®é™…æ›´æ–°é€»è¾‘æœ‰ä¸€äº›æ¡ä»¶åˆ¤æ–­ã€‚

è®©æˆ‘æŒ‡å‡ºä»£ç ä¸­çš„å…³é”®éƒ¨åˆ†ï¼š

```python
# ç¬¬ 807-822 è¡Œ

# æ›´æ–° Critic
if self.use_critic:  # â† æ¡ä»¶1: ä½¿ç”¨ Critic (GAE æ¨¡å¼)
    with _timer('update_critic', timing_raw):
        critic_output = self.critic_wg.update_critic(batch)
    critic_output_metrics = reduce_metrics(critic_output.meta_info['metrics'])
    metrics.update(critic_output_metrics)

# Critic é¢„çƒ­æœºåˆ¶
if self.config.trainer.critic_warmup <= self.global_steps:  # â† æ¡ä»¶2: é¢„çƒ­ç»“æŸåæ‰æ›´æ–° Actor
    # æ›´æ–° Actor
    with _timer('update_actor', timing_raw):
        if self.config.do_search and self.config.actor_rollout_ref.actor.state_masking:
            batch, metrics = self._create_loss_mask(batch, metrics)
        actor_output = self.actor_rollout_wg.update_actor(batch)
    actor_output_metrics = reduce_metrics(actor_output.meta_info['metrics'])
    metrics.update(actor_output_metrics)
```

---

### æ›´æ–°é€»è¾‘æ€»ç»“

| ç»„ä»¶ | æ˜¯å¦æ›´æ–° | æ¡ä»¶ |
|------|---------|------|
| **Critic** | âœ… æœ‰æ¡ä»¶ | `self.use_critic = True`ï¼ˆä½¿ç”¨ GAE ä¼˜åŠ¿ä¼°è®¡æ—¶ï¼‰ |
| **Actor** | âœ… æœ‰æ¡ä»¶ | `critic_warmup <= global_steps`ï¼ˆCritic é¢„çƒ­å®Œæˆåï¼‰ |

---

### ä¸¤ç§æ¨¡å¼çš„åŒºåˆ«

#### 1. **GAE æ¨¡å¼** (`adv_estimator == 'gae'`)
- **éœ€è¦ Critic**: `use_critic = True`
- **æ›´æ–°é¡ºåº**: Critic å…ˆæ›´æ–° â†’ Actor åæ›´æ–°
- **Critic é¢„çƒ­**: å‰ N æ­¥åªæ›´æ–° Criticï¼Œä¸æ›´æ–° Actor

#### 2. **GRPO æ¨¡å¼** (`adv_estimator == 'grpo'`)
- **ä¸éœ€è¦ Critic**: `use_critic = False`
- **åªæ›´æ–° Actor**: ä¼˜åŠ¿ç›´æ¥ä» group reward è®¡ç®—

```python
# ç¬¬ 567-576 è¡Œ
if self.config.algorithm.adv_estimator == 'gae':
    # ... åˆ›å»º Critic
    self.use_critic = True
elif self.config.algorithm.adv_estimator == 'grpo':
    self.use_critic = False  # GRPO ä¸ä½¿ç”¨ Critic
```

---

### Critic é¢„çƒ­æœºåˆ¶çš„ä½œç”¨

```python
if self.config.trainer.critic_warmup <= self.global_steps:
```

**ç›®çš„**: åœ¨è®­ç»ƒåˆæœŸï¼ŒCritic çš„ä»·å€¼ä¼°è®¡ä¸å‡†ç¡®ã€‚é€šè¿‡å…ˆè®­ç»ƒå‡ æ­¥ Criticï¼ˆä¸æ›´æ–° Actorï¼‰ï¼Œå¯ä»¥è®© Critic å…ˆå­¦ä¼šæ›´å‡†ç¡®çš„ä»·å€¼ä¼°è®¡ï¼Œç„¶åå†ç”¨è¿™ä¸ªæ›´å‡†ç¡®çš„ä¼°è®¡æ¥æŒ‡å¯¼ Actor çš„ç­–ç•¥æ›´æ–°ã€‚

è¿™æ˜¯ PPO è®­ç»ƒçš„å¸¸è§æŠ€å·§ï¼Œå¯ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚

---

### é—®é¢˜ 4
ç”¨æˆ·ï¼šä¸ºäº†æŒæ¡RLï¼Œæ˜¯ä¸æ˜¯å°†æ­¤training loopç ”è¯»å®Œå°±å¯ä»¥äº†
åŠ©æ‰‹ï¼šç ”è¯»è¿™ä»½ `ray_trainer.py` æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œä½†**è¿˜ä¸è¶³ä»¥å®Œå…¨æŒæ¡ RL**ã€‚è¿™ä»½ä»£ç æ›´å¤šæ˜¯å…³äº**å·¥ç¨‹å®ç°**ï¼Œè€Œé**æ ¸å¿ƒç®—æ³•åŸç†**ã€‚

---

### è¿™ä»½ä»£ç èƒ½å­¦åˆ°ä»€ä¹ˆ âœ…

| å†…å®¹ | ä»£ç ä½ç½® |
|------|----------|
| PPO è®­ç»ƒæµç¨‹ | `fit()` æ–¹æ³• |
| ä¼˜åŠ¿ä¼°è®¡ (GAE/GRPO) | `compute_advantage()` |
| KL æƒ©ç½šæœºåˆ¶ | `apply_kl_penalty()` |
| åˆ†å¸ƒå¼è®­ç»ƒæ¶æ„ | `RayWorkerGroup` åè°ƒ |
| æ•°æ®æµç®¡ç† | `DataProto` æ“ä½œ |

---

### è¿˜éœ€è¦è¡¥å……çš„æ ¸å¿ƒç®—æ³•çŸ¥è¯† ğŸ“š

è¦çœŸæ­£æŒæ¡ RLï¼Œè¿˜éœ€è¦ç ”è¯» **`core_algos.py`**ï¼Œè¿™æ˜¯æ ¸å¿ƒç®—æ³•å®ç°ï¼š

```python
from verl.trainer.ppo import core_algos

# å…³é”®ç®—æ³•å‡½æ•°
core_algos.compute_gae_advantage_return(...)  # GAE ä¼˜åŠ¿è®¡ç®—
core_algos.compute_policy_loss(...)            # PPO ç­–ç•¥æŸå¤±
core_algos.compute_value_loss(...)             # Critic ä»·å€¼æŸå¤±
core_algos.kl_penalty(...)                     # KL æ•£åº¦æƒ©ç½š
```

---

### æ¨èçš„ RL å­¦ä¹ è·¯å¾„

#### ç¬¬ä¸€å±‚ï¼šæ ¸å¿ƒç®—æ³•ï¼ˆå¿…è¯»ï¼‰

| æ–‡ä»¶ | å†…å®¹ | ä¼˜å…ˆçº§ |
|------|------|--------|
| `core_algos.py` | PPO æ ¸å¿ƒç®—æ³•å®ç° | â­â­â­ |
| â†³ `compute_gae_advantage_return()` | GAE ä¼˜åŠ¿ä¼°è®¡ | |
| â†³ `compute_policy_loss()` | PPO ç­–ç•¥æŸå¤±ï¼ˆClipped Objectiveï¼‰ | |
| â†³ `compute_value_loss()` | Critic ä»·å€¼æŸå¤± | |
| â†³ `kl_penalty()` | KL æ•£åº¦æƒ©ç½š | |

#### ç¬¬äºŒå±‚ï¼šæ¨¡å‹æ›´æ–°é€»è¾‘

| æ–‡ä»¶ | å†…å®¹ |
|------|------|
| `actor/fsdp_workers.py` | Actor æ¨¡å‹æ›´æ–°å®ç° |
| `critic/fsdp_workers.py` | Critic æ¨¡å‹æ›´æ–°å®ç° |

#### ç¬¬ä¸‰å±‚ï¼šç¯å¢ƒäº¤äº’ï¼ˆæœ¬é¡¹ç›®ç‰¹è‰²ï¼‰

| æ–‡ä»¶ | å†…å®¹ |
|------|------|
| `generation.py` | LLM Agent ç”Ÿæˆå¾ªç¯ |
| Reward Function | å¥–åŠ±è®¾è®¡ |

---

### å­¦ä¹ å»ºè®®

```
ç†è®ºåŸºç¡€          å®ç°ç†è§£           å·¥ç¨‹èƒ½åŠ›
    â”‚                 â”‚                â”‚
    â–¼                 â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚PPO è®ºæ–‡ â”‚ â”€â”€â–¶ â”‚ core_algos.pyâ”‚â”€â”€â–¶â”‚ray_trainer.pyâ”‚
â”‚GAE è®ºæ–‡ â”‚     â”‚              â”‚  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æˆ‘çš„å»ºè®®**ï¼š

1. **å…ˆçœ‹ `core_algos.py`** - ç†è§£ GAEã€Policy Lossã€Value Loss çš„æ•°å­¦å®ç°
2. **å†å›çœ‹ `ray_trainer.py`** - ç†è§£æ•°æ®æµå¦‚ä½•æµç» these ç®—æ³•
3. **é˜…è¯» PPO åŸè®ºæ–‡** - [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
4. **é˜…è¯» GAE è®ºæ–‡** - [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)

åªçœ‹ `ray_trainer.py` æ›´å¤šæ˜¯å­¦ä¼š"å¦‚ä½•ç”¨ RL"ï¼Œè€Œé"RL ä¸ºä»€ä¹ˆè¿™æ ·å·¥ä½œ"ã€‚
